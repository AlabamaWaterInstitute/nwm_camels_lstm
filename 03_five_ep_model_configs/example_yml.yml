# --- Experiment configurations --------------------------------------------------------------------

# experiment name, used as folder name
experiment_name: nwm-analysis-conus

run_dir: ../runs/nwm-conus

# files to specify training, validation and test basins (relative to code root or absolute path)
train_basin_file: /media/volume/NeuralHydrology/neuralhydrology/nwm-analysis/nwm-data/conus_runs/train0502.txt
validation_basin_file: /media/volume/NeuralHydrology/neuralhydrology/nwm-analysis/nwm-data/conus_runs/val0502.txt
test_basin_file: /media/volume/NeuralHydrology/neuralhydrology/nwm-analysis/nwm-data/conus_runs/test0502.txt

# training, validation and test time periods (format = 'dd/mm/yyyy')
train_start_date: '01/10/1979'
train_end_date: '30/09/2004'
validation_start_date: '01/10/2003'
validation_end_date: '30/09/2008'
test_start_date: '01/10/2008'
test_end_date: '30/09/2011'

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu, mps or lsNone]
device: cuda:0

# --- Validation configuration ---------------------------------------------------------------------

# specify after how many epochs to perform validation
validate_every: 1

# specify how many random basins to use for validation
validate_n_random_basins: 3

# specify which metrics to calculate during validation (see neuralhydrology.evaluation.metrics)
# this can either be a list or a dictionary. If a dictionary is used, the inner keys must match the name of the
# target_variable specified below. Using dicts allows for different metrics per target variable.
metrics:
  - NSE

# --- Model configuration --------------------------------------------------------------------------

# base model type [lstm, ealstm, cudalstm, embcudalstm, mtslstm]
# (has to match the if statement in modelzoo/__init__.py)
model: modifiedcudalstm

# prediction head [regression]. Define the head specific parameters below
head: regression

# ----> Regression settings <----
output_activation: linear

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 20

# Initial bias value of the forget gate
initial_forget_bias: 3

# Dropout applied to the output of the LSTM
output_dropout: 0.4

# ----> Embedding network settings <----

# define embedding network for static inputs
# statics_embedding:
#   type: fc
#   # define number of neurons per layer in the FC network used as embedding network
#   hiddens:
#     - 30
#     - 20
#     - 64
#   # activation function of embedding network
#   activation: tanh
#   # dropout applied to embedding network
#   dropout: 0.0

# define embedding network for dynamic inputs
dynamics_embedding:
  type: fc
  # define number of neurons per layer in the FC network used as embedding network
  hiddens:
    - 30
    - 20
    - 64
  # activation function of embedding network
  activation: tanh
  # dropout applied to embedding network
  dropout: 0.0

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam]
optimizer: Adam

# specify loss [MSE, NSE, RMSE]
loss: MSE

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
  0: 8e-4

# Mini-batch size
batch_size: 512

# Number of training epochs
epochs: 5

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1

# Defines which time steps are used to calculate the loss. Can't be larger than seq_length.
# If use_frequencies is used, this needs to be a dict mapping each frequency to a predict_last_n-value, else an int.
predict_last_n: 1

# Length of the input sequence
# If use_frequencies is used, this needs to be a dict mapping each frequency to a seq_length, else an int.
seq_length: 1024

# Number of parallel workers used in the data pipeline
# num_workers: 30

# Log the training loss every n steps
log_interval: 20

# If true, writes logging results into tensorboard file
log_tensorboard: False

# If a value and greater than 0, logs n random basins as figures during validation
log_n_figures: 1

# Save model weights every n epochs
save_weights_every: 1

# --- Data configurations --------------------------------------------------------------------------

# which data set to use [camels_us, camels_gb, global, hourly_camels_us]
dataset: nwm3retro

# Path to data set root
data_dir: /media/volume/NeuralHydrology/Test_Quinn_Data/CAMELS_data

# Forcing product [daymet, maurer, maurer_extended, nldas, nldas_extended, nldas_hourly]
# can be either a list of forcings or a single forcing product
dynamic_inputs:
# - DLWRF_surface
# - PRES_surface
# - VGRD_10maboveground
# - UGRD_10maboveground
# - DSWRF_surface
- precip_rate
# - DLWRF_surface_d
# - PRES_surface_d
# - VGRD_10maboveground_d
# - UGRD_10maboveground_d
# - DSWRF_surface_d
- precip_rate_d
- streamflow_d

# which columns to use as target
target_variables:
- streamflow

# clip negative predictions to zero for all variables listed below. Should be a list, even for single variables.
clip_targets_to_zero:
- streamflow

# Which CAMELS attributes to use. Leave empty if none should be used
# static_attributes:
# - basin_area
# - basin_length
# - reach_length
# - basin_area_d
# - basin_length_d
# - reach_length_d



# --- Miscellaneous configuration ------------------------------------------------------------------
early_stopping: True   # if True, training will stop if validation loss does not improve for n epochs
patience_early_stopping: 10 #number of subsequent epochs to wait before stopping
minimum_epochs_before_early_stopping: 50 # minimum number of epochs before early stopping triggers
dynamic_learning_rate: True # if True, learning rate will be reduced if training loss does not improve for n epochs
patience_dynamic_learning_rate: 3 # number of subsequent epochs to wait before reducing learning rate
factor_dynamic_learning_rate: 0.8 # factor by which to reduce learning rate
